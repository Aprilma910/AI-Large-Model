Q：autodl跑汽车剐蹭视频理解，提示cuda out of memory，是显存不够吗，该怎么解决，我用的是4096的
需要清理掉之前已经在使用显存的程序，4090 单独跑这个视频理解 是够的

Q：老师，Agent智能体中的agent QA测评，其数据是excel格式吗，就是里面包含两列，一列Q一列A？
是的，
Q，A 这里的A是我们认为标准的回答，与 AI回答进行对比，看是否有冲突

Q：陈博士，能否讲下cursor如何连接autodl全过程，今天安装半天也没成功。
cursor是本地的软件，帮你写代码
你可以把写好的代码，放到 autodl（线上的云端jupyter）进行运行

需要安装Flash-atten 但是编译好费时间，我这搞了好久

Q：老师，汽车视频，我的视频说明，直接是英文的，是哪里控制的？后边问答是中文的。
可以在提示词中说明用中文回复

Q：陈博，视频识别AttributeError: 'InternVLChatConfig' object has no attribute 'llm_config
#model_dir = snapshot_download('internlm/internlm2_5-7b-chat', cache_dir='/root/autodl-tmp/models')


Q：假如针对deepseek 进行微调的话，咋样在大模型输出之前把我的参数影响带上呢
如果使用LoRA进行微调，会得到一个 额外的  LoRA模型，
假设原尺寸是 7B（70亿参数）的，LoRA模型 只有大约7000万参数

下次使用推理的时候，需要加载 基座模型（7B） +  LoRA（7000万参数）
input => 经过 7B + 7000万的参数 前向传播 => 原始deepseek + lora调整 => 微调后的结果

Q：大厂训练模型，他们也会用这种矩阵分解的方法来减少参数量吗
 
https://github.com/huggingface/peft

Q：那跟蒸馏有什么区别？
LoRA 微调微调的训练，主要是增加一个 小的旁支，来得到 增量的微调结果

蒸馏，将大尺寸模型的能力 迁移到 小尺寸模型上；
用大尺寸（70B）模型模的 <input, output> 作为训练集，让小尺寸模型（7B）进行学习

Q：蒸馏是不是可以理解成全参数微调
整理是一种学习迁移，将大模型的能力迁移到小尺寸模型

蒸馏是训练样本上的迁移，lora是对大模型参数的近似迁移

lora参数是和原decode结构中的每一个W权重矩阵一一对应的吗

Q: 是每一层神经网络都进行单独矩阵分解吗？

公开的数据源
https://huggingface.co/

Q：7B的训练集、验证集、测试集大概需要多少条？
如果难度不是很大，可以先准备1000条高质量的样本
训练集与验证集的比例 = 7:3 

Q：验证集与训练集的差异，如何把控？
多样性 样本均衡的原则

数据集来源也可以查看
https://modelscope.cn/home

Q：老师，一般企业大部分情况下用不到微调吧？是不是通过RAG和数据训练能达到预期效果。除非要改变风格之类的
对 比例不是 很高
